{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cc98af",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1738e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Dynamic Programming\" # Change the Input to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5eb2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Joshua\\.cache\\huggingface\\hub\\models--markteammate--Mistral_academic_style_tune. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write an academic paragraph given the title.\n",
      "\n",
      "### Input: Dynamic Programming\n",
      "\n",
      "### Response:\n",
      "Dynamic programming is a method for solving optimization problems. It is a recursive method. It is used to solve many problems in computer science and operations research. It is widely used in many areas such as optimization, machine learning, artificial intelligence and operations research. This paper presents the dynamic programming method with a mathematical definition. And the concept of the dynamic programming method is presented with a graphical and geometric explanation. A simple dynamic programming example is presented. And then two optimization problems, the Traveling salesman problem and the knapsack problem are solved with dynamic programming. The method is used in other optimization problems. The method is also used in the solution of the knapsack problem and the traveling salesman problem. The method is also used in other\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "lora_repo = \"markteammate/Mistral_academic_style_tune\"\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === QLoRA quantization config ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# === Load base model with quantization ===\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# === Load LoRA adapter from Hugging Face ===\n",
    "model = PeftModel.from_pretrained(base_model, lora_repo)\n",
    "model.eval()\n",
    "\n",
    "# === Prompt ===\n",
    "prompt = \"\"\"### Instruction:\n",
    "Write an academic paragraph given the title.\n",
    "\n",
    "### Input: \"\"\" + input + \"\"\"\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# === Tokenize and generate ===\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "# === Decode ===\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
