{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbc572e",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Deep neural network\" # Change the prompt to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26390b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "Deep neural network (DNN) models can be used to effectively predict the future of an unknown system. However, existing methods typically only consider a single prediction function and do not generalize well across different scenarios. To address this gap between training data sets for specific tasks in DNNs that are directly related with predictions derived from other parameters or domains such as model labels during inference time rather than solely focusing on one parameter domain without considering more information about another task at hand is critical; it has been shown experimentally how various classifier-based approaches improve accuracy by varying their weights over multiple datasets while still maintaining high precision within certain settings compared\n",
      "The recent discovery made possible new directions towards quantum gravity. We report here the study performed using polarized beam epitaxy spectroscopy\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"markteammate/GPT-2_academic_style_tune\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"markteammate/GPT-2_academic_style_tune\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
